---
layout: default
---

## 计算机基本结构

计算机从出现至今，一直使用冯诺依曼提出的现代计算机结构模型，分为运算器、控制器、存储器、输入设备、输出设备几个组件。

运算器和控制器为计算机的CPU，包括数字计算、逻辑计算、控制等。

存储器为计算机的**内存**，实际上还包括了位于CPU上的高速缓存，以及作为外存储器的硬盘、闪存等。

输入设备为计算机提供输入，如键盘、鼠标等。

输出设备为计算机提供输出，如显示器、音响、打印机等。

## 二进制运算与编码

我们平时使用的数学运算是"十进制运算"，其特点是"逢十进一"。
对于计算机来说，设计出0~9这么多个数字过于复杂且没有收益，计算机在内部通常使用二进制的方式进行数字的存储和运算。
二进制的特点是仅由数字0和1组成，逢二进一。可以在各种硬件设备中使用高低电位代表0和1。

在计算机中，常用的除了二进制数外，还有十六进制数。因为二进制数写出来是一大堆0和1，通常很长不容易看，而$$16=2^4$$，
十六进制数和二进制数可以很方便的进行转换，所以十六进制数也比较常用。

我们经常看到的文件校验码、UUID、激活码等，很多都是十六进制数。

| 十进制数 | 二进制数 | 十六进制数 |
| -------: | -------: | ---------: |
|        0 |        0 |          0 |
|        1 |        1 |          1 |
|        2 |       10 |          2 |
|        3 |       11 |          3 |
|        4 |      100 |          4 |
|        5 |      101 |          5 |
|        6 |      110 |          6 |
|        7 |      111 |          7 |
|        8 |     1000 |          8 |
|        9 |     1001 |          9 |
|       10 |     1010 |          a |
|       11 |     1011 |          b |
|       12 |     1100 |          c |
|       13 |     1101 |          d |
|       14 |     1110 |          e |
|       15 |     1111 |          f |
|       16 |    10000 |         10 |
|       17 |    10001 |         11 |
|       18 |    10010 |         12 |
|       19 |    10011 |         13 |
|       20 |    10100 |         14 |

二进制的运算除了普通的加减乘除外，还有与、或、取反、左移、右移等。负数使用补码方式表示。

浮点数通常遵循[IEEE 754标准](https://en.wikipedia.org/wiki/IEEE_754)。

## Bit, Byte

二进制中的一位称为一个Bit（比特）。八个二进制位为一个Byte。

一个Byte（字节）的表示范围为$$[0, 255]$$。

字节是在计算机中最为重要的单位。由于一个Bit能够表示的范围太小，在绝大多数场景下没有意义，计算机使用字节作为最小操作单元。

我们之前提到的变量类型，即使是只有$${true, false}$$两个可能的值的`bool`类型，也会占用一个Byte进行存储。

## 位运算

位运算是一些在二进制表示方式上的运算，包括按位与`&`，按位或`|`，异或`^`，按位取反`~`，左移`<<`，右移`>>`。

位运算在某些情况下非常有用，我们会在之后继续讨论到他们。

之前提到`bool`类型会占用一个Byte，那么如果我们想声明数组`bool a[100]`，会占用多少内存呢？

答案是会占用100个字节。但实际上，由于`bool`类型只有两种值，可以用一个Bit来表示，
我们可以使用位运算的方式，仅使用`(100+7)/8=13`个字节，用`byte a[13]`来代替。

## 大端和小端

如`int`类型占用的内存是四个字节的，而内存的最小单位是一个字节。那么超过255的数字，在计算机的内存中，是怎么存储的呢？

我们以数字300为例，转为十六进制是`0x000012c`。这里`0x`开头表示接下来是一个16进制数。十六进制的两个数字位是一个字节。
类似的还有`0`开头表示接下来是一个8进制数。

在计算机硬件设备的研制过程中，不同的公司采用了不同的方法存储这样的数字。

一种常见的存放方法是`00 00 01 2c`，这种存放方法属于大端派。
另一种存放方法是`2c 01 00 00`，这种存放方法属于小端派。

小端派看起来好像比较反人类，但实际使用时，我们大多数场景不需要关注这个数字在内存和处理器中是如何存储的。

英特尔处理器是小端派。

大端和小端对我们最大的影响是，在数据传输/存储时，对方的计算机可能和你的不是一个流派的。
所以协议本身必须规定好是大端还是小端，而不能直接把内存中的内容扔进去。

## ASCII编码

前面一直在讨论数字计算，那么对于字符，计算机是怎么处理的呢？计算机使用一定的**编码规则**来把数字映射到对应的字符上。

一个比较古老也比较通用的编码方式，就是[ASCII编码](这里放维基百科)(American Standard Code for Information Interchange)。

实际上"ASCII编码"这个稍微有点语病，因为"C"表示Code，已经有编码的含义在里边了...类似的语病还有"HTTP协议"等。

ASCII编码把0~127映射到常用的128个字符上，包括英文小写字母、英文大写字母、数字、空白符、控制字符等。

后来人们又对ASCII编码进行了扩展，变为0~255一共256个字符。

## UNICODE与UTF-8

即使是扩展过后的ASCII编码，也只能支持256种字符，显然无法支撑像中文这种庞大的字符集。

这时各个国家分别制定了对于自己的编码规则，如支持中文的GBK编码。

但各个国家的编码不统一，互相之间传输数据时，还要告诉对方自己是什么编码，很是麻烦。
于是就有人编出了UNICODE编码，希望把各个国家的字符都支持进来。

UNICODE使用两个字节表示一个字符，这样就可以支撑$$2^16=65536$$种字符。中文、日文等我们能想象到的字符都被包括了进来。

但有一个很大的问题是，UNICODE编码和ASCII编码不兼容。使用ASCII编码写好的文件，当做UNICODE编码方式打开的话，几乎一定会变成乱码。
另外还有一个问题是，最常用的英文字符，为了支持其他语言，每个字符占用的空间从一个字节变为了两个字节，文件体积增大了一倍，对存储和数据传输都不友好。

于是又有人想出了UTF-8 (8-bit Unicode Transformation Format)编码。
这种编码方式是**变长**编码，对于原始的ASCII编码支持的128个字符，它仍使用一个字节表示。

对于大于127的字符，他使用如下规则：
第一个字节的前n位为1，接下来一个0，表示一共有n个字节，剩余比特用来表示字符原本内容。
其余所有字节最高位为1，接下来一个0，剩余比特用来表示字符原本内容。
把这些"字符剩余原本内容"连接起来之后，就是UNICODE编码的字符了。

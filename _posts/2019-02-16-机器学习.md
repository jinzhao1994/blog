---
layout: default
tags: [Coursera, 学习笔记, 机器学习]
---

## 变量名称表格

{:.compact-table}
| 变量              | 含义                                          |
| :---------------- | :-------------------------------------------- |
| \\( \theta    \\) | 模型的参数                                    |
| \\( x         \\) | 输入数据（特征）                              |
| \\( h_\theta  \\) | 输出数据（预测值）                            |
| \\( y         \\) | 实际值                                        |
| \\( m         \\) | 训练数据个数                                  |
| \\( J(\theta) \\) | 总损失（误差）                                |
| \\( \alpha    \\) | 学习速率                                      |
| \\( n         \\) | 特征数目                                      |
| \\( x_j^{(i)} \\) | 第$$i$$个样本的第$$j$$个特征                  |
| \\( x^{(i)}   \\) | 第$$i$$个样本，共$$n+1$$行的列向量            |
| \\( X         \\) | 所有$$m$$个样本的特征，$$m$$行$$n+1$$列的矩阵 |

## Week1

**线性回归算法**处理预测问题。

\\[ h_{\theta}(x) = \theta_0 + \theta_1x \\]

\\[ J(\theta)=\frac{1}{2m} \sum_{i=1}^{m} (h_{\theta}(x_i)-y_i)^2 \\]

使用梯度下降的方式让总误差尽可能的小：

\\[ \theta_j := \theta_j - \alpha\frac{\partial}{\partial\theta_j}J(\theta) \\]

整理可以得到：

\\[ \theta_0 := \theta_0 - \alpha\frac{1}{m} \sum_{i=1}^{m} (h_\theta(x_i)-y_i) \\]
\\[ \theta_1 := \theta_1 - \alpha\frac{1}{m} \sum_{i=1}^{m} (h_\theta(x_i)-y_i)x_i \\]

## Week2

当有多个特征时，使用列向量$$x$$和$$\theta$$可以简化计算。

$$
h_{\theta}(x)
    = \begin{bmatrix}
        \theta_0 & \theta_1 & \cdots & \theta_n
    \end{bmatrix} \begin{bmatrix}
        x_0 \\
        x_1 \\
        \vdots \\
        x_n
    \end{bmatrix}
    = \theta^Tx
$$

通过**特征缩放**和**均值归一化**使所有特征的输入范围相同可以让梯度下降更快的计算出结果。通常使用这样的方式处理特征。
\\[ x_i := \frac{x_i-\mu_i}{s_i} \\]
其中，$$\mu_i$$为第$$i$$个特征的均值，$$s_i$$为第$$i$$个特征的范围（最大值减最小值）或标准差。

学习率过低会导致训练速度过慢，学习率过高可能会导致不收敛。

当线性不足以描述输入与输出之间的关系时，可以手动处理一些特征（如乘积、根号等）。

由于线性回归的特点，整理可以得出公式解：
\\[ \theta=(X^TX)^{-1}X^Ty \\]
使用公式解无需考虑学习速率$$\alpha$$，无需处理特征缩放，但需要$$O(n^3)$$的复杂度计算逆元。

## Week3

**逻辑回归算法**处理分类问题。

在二分类问题中，$$y \in {0, 1}$$。

\\[ h_{\theta}(x) = g(\theta^Tx) = g(z) = \frac{1}{1+e^{-z}} \\]
其中$$g(z) = \frac{1}{1+e^{-z}}$$称为sigmoid函数，将$$(-\infty,\infty)$$映射为$$(0,1)$$。 $$h_\theta(x)$$表示分为某一类的概率。

\\[ J(\theta) = \frac{1}{m} \sum_{i=1}^{m} Cost(h_\theta(x^{(i)}), y^{(i)}) \\]
\\[ J(\theta) = \frac{1}{m} \sum_{i=1}^{m} [-y^{(i)}\log(h_\theta(x^{(i)})) - (1-y^{(i)})\log(1-h_\theta(x^{(i)}))] \\]

向量化后可以表示为

\\[ h = g(X\theta) \\]
\\[ J(\theta) = \frac{1}{m} (-y^T\log(h) - (1-y)^T\log(1-h)) \\]

梯度下降公式为

\\[ \theta_j := \theta_j - \alpha\frac{\partial}{\partial\theta_j}J(\theta) \\]
\\[ \theta_j := \theta_j - \frac{\alpha}{m} \sum_{i=1}^{m}(h_\theta(x^{(i)})-y^{(i)})x_j^{(i)} \\]

## Week4

## Week5

## Week6

## Week7

## Week8

## Week9

## Week10

## Week11
